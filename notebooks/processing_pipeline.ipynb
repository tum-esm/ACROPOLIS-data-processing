{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import plotly.express as px\n",
    "\n",
    "from download import extract\n",
    "from utils import processing_utils as pu\n",
    "\n",
    "DATA_DIRECTORY = os.environ.get(\"DATA_DIRECTORY\")\n",
    "PICARRO_DATA_DIRECTORY = os.environ.get(\"PICARRO_DATA_DIRECTORY\")\n",
    "\n",
    "sensor_id = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,18,20]\n",
    "\n",
    "# customize pipeline\n",
    "download_files = True\n",
    "merge_picarro_files = False\n",
    "\n",
    "# load calibration bottle concentrations (preprocessed)\n",
    "df_gas = pl.read_csv(os.path.join(DATA_DIRECTORY,\"input\", \"averaged_gases.csv\"))\n",
    "# load local db: acropolis.parquet\n",
    "df = pl.scan_parquet(os.path.join(DATA_DIRECTORY, \"download\", \"acropolis.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download to local db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start downloading from datetime: \n",
      "2024-02-20 12:56:14.221898+00:00\n",
      "2024-02-20 13:12:47.262879+00:00\n",
      "2024-02-20 13:29:18.085299+00:00\n",
      "2024-02-20 13:45:49.170653+00:00\n",
      "2024-02-20 14:02:17.614481+00:00\n",
      "2024-02-20 14:18:06.463232+00:00\n",
      "2024-02-20 14:33:18.381416+00:00\n",
      "2024-02-20 14:48:46.930536+00:00\n",
      "2024-02-20 15:03:30.274214+00:00\n",
      "2024-02-20 15:18:41.264162+00:00\n",
      "2024-02-20 15:33:29.041352+00:00\n",
      "2024-02-20 15:48:11.754161+00:00\n",
      "2024-02-20 16:03:12.297150+00:00\n",
      "2024-02-20 16:17:54.352815+00:00\n",
      "2024-02-20 16:32:45.099620+00:00\n",
      "2024-02-20 16:47:30.413754+00:00\n",
      "2024-02-20 17:02:15.479428+00:00\n",
      "2024-02-20 17:17:01.004375+00:00\n",
      "2024-02-20 17:31:44.059652+00:00\n",
      "2024-02-20 17:46:28.901341+00:00\n",
      "2024-02-20 18:01:13.282297+00:00\n",
      "2024-02-20 18:15:58.436474+00:00\n",
      "2024-02-20 18:30:41.573054+00:00\n",
      "2024-02-20 18:45:25.192721+00:00\n",
      "2024-02-20 19:00:10.440829+00:00\n",
      "2024-02-20 19:14:53.109382+00:00\n",
      "2024-02-20 19:29:38.781256+00:00\n",
      "2024-02-20 19:44:22.611697+00:00\n",
      "2024-02-20 19:59:03.030723+00:00\n",
      "2024-02-20 20:13:51.518960+00:00\n",
      "2024-02-20 20:28:35.325611+00:00\n",
      "2024-02-20 20:43:19.297543+00:00\n",
      "2024-02-20 20:58:03.817364+00:00\n",
      "2024-02-20 21:12:46.348897+00:00\n",
      "2024-02-20 21:27:31.992444+00:00\n",
      "2024-02-20 21:42:18.242503+00:00\n",
      "2024-02-20 21:57:01.870478+00:00\n",
      "2024-02-20 22:11:48.464843+00:00\n",
      "2024-02-20 22:26:36.290162+00:00\n",
      "2024-02-20 22:41:20.458975+00:00\n",
      "2024-02-20 22:56:11.067649+00:00\n",
      "2024-02-20 23:11:00.070970+00:00\n",
      "2024-02-20 23:25:45.601845+00:00\n",
      "2024-02-20 23:40:32.140397+00:00\n",
      "2024-02-20 23:55:20.670214+00:00\n",
      "2024-02-21 00:10:05.678926+00:00\n",
      "2024-02-21 00:24:55.845026+00:00\n",
      "2024-02-21 00:39:43.752192+00:00\n",
      "2024-02-21 00:54:27.753596+00:00\n",
      "2024-02-21 01:09:14.328322+00:00\n",
      "2024-02-21 01:24:00.492708+00:00\n",
      "2024-02-21 01:38:44.612528+00:00\n",
      "2024-02-21 01:53:29.097806+00:00\n",
      "2024-02-21 02:08:16.878773+00:00\n",
      "2024-02-21 02:23:02.032711+00:00\n",
      "2024-02-21 02:37:49.622311+00:00\n",
      "2024-02-21 02:52:35.134713+00:00\n",
      "2024-02-21 03:07:17.380772+00:00\n",
      "2024-02-21 03:22:07.941810+00:00\n",
      "2024-02-21 03:39:42.333689+00:00\n",
      "2024-02-21 03:54:29.015750+00:00\n",
      "2024-02-21 04:09:14.478624+00:00\n",
      "2024-02-21 04:24:00.708900+00:00\n",
      "2024-02-21 04:38:44.727617+00:00\n",
      "2024-02-21 04:53:34.912811+00:00\n",
      "2024-02-21 05:08:20.564988+00:00\n",
      "2024-02-21 05:23:12.597774+00:00\n",
      "2024-02-21 05:37:59.116816+00:00\n",
      "2024-02-21 05:52:46.532704+00:00\n",
      "2024-02-21 06:07:33.280451+00:00\n",
      "2024-02-21 06:22:19.221643+00:00\n",
      "2024-02-21 06:37:05.284983+00:00\n",
      "2024-02-21 06:51:52.833981+00:00\n",
      "2024-02-21 07:06:37.296052+00:00\n",
      "2024-02-21 07:21:22.921969+00:00\n",
      "2024-02-21 07:36:12.703849+00:00\n",
      "2024-02-21 07:50:55.977047+00:00\n",
      "2024-02-21 08:05:48.181884+00:00\n",
      "2024-02-21 08:20:34.076618+00:00\n",
      "2024-02-21 08:35:19.127619+00:00\n",
      "2024-02-21 08:50:04.845825+00:00\n",
      "2024-02-21 09:04:57.866770+00:00\n",
      "2024-02-21 09:19:51.933079+00:00\n",
      "2024-02-21 09:34:40.846399+00:00\n",
      "2024-02-21 09:49:46.763579+00:00\n",
      "Performing merge.\n",
      "Deleting merged chunks.\n"
     ]
    }
   ],
   "source": [
    "# download from hermes database\n",
    "# Use Download/download_from_hermes notebook\n",
    "if download_files:\n",
    "    component = extract.Extract()\n",
    "    result = component.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Picarro Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_picarro_files:\n",
    "    filenames = glob.glob(PICARRO_DATA_DIRECTORY + \"/*/*/*.dat\")\n",
    "\n",
    "    # read all *.dat picarro measurement files and add to single db\n",
    "    df_list = []\n",
    "    for filename in filenames:\n",
    "        df_list.append(pd.read_csv(filename,sep='\\s+'))\n",
    "\n",
    "    df_p_files = pd.concat(df_list, ignore_index=True)\n",
    "    df_p_files[\"datetime\"] = pd.to_datetime((df_p_files['DATE'] + ' ' + df_p_files['TIME']))\n",
    "    df_p_files.sort_values(by='datetime', inplace = True)\n",
    "\n",
    "    df_p_files.to_parquet(path = os.path.join(DATA_DIRECTORY, \"input\", \"picarro.parquet\"))\n",
    "\n",
    "    #Calibration\n",
    "\n",
    "    # TODO: Add ability for multiple calibration dates\n",
    "    # before 23.10\n",
    "    # picarro_slope = 1.0061589132696314\n",
    "    # picarro_intercept = 0.14607153970888476\n",
    "\n",
    "    # after 23.10\n",
    "    #picarro_slope = 1.006374633215469\n",
    "    #picarro_intercept = 0.0709482571842841\n",
    "    \n",
    "    #after 18.12\n",
    "    picarro_slope = 1.0060429925902534 \n",
    "    picarro_intercept = 0.09305508001614271\n",
    "\n",
    "    #1h averaged corrected Picarro dataset\n",
    "    df_p_1h = pl.scan_parquet(os.path.join(DATA_DIRECTORY,\"input\", \"picarro.parquet\")) \\\n",
    "        .with_columns(pl.col(\"datetime\").dt.cast_time_unit(\"us\").dt.replace_time_zone(\"UTC\").alias(\"creation_timestamp\")) \\\n",
    "        .sort(\"creation_timestamp\") \\\n",
    "        .with_columns((pl.col(\"CO2_dry\") * picarro_slope + picarro_intercept).alias(\"CO2_corr\")) \\\n",
    "        .groupby_dynamic(\"creation_timestamp\", every='1h') \\\n",
    "        .agg(pl.all().exclude(\"creation_timestamp\").mean()).collect() \\\n",
    "        .select([\"creation_timestamp\", \"CO2_corr\", \"h2o_reported\"]) \\\n",
    "        .with_columns([pl.lit(picarro_slope).alias(\"slope\"),\n",
    "            pl.lit(picarro_intercept).alias(\"intercept\"),\n",
    "            pl.lit(\"Picarro\").alias(\"system_name\"),\n",
    "            pl.lit(0.0).alias(\"diff\")]\n",
    "            ) \\\n",
    "\n",
    "    df_p_1h.write_parquet(os.path.join(DATA_DIRECTORY,\"processed\", \"1h_cal_corr_picarro.parquet\"))\n",
    "\n",
    "    #10m averaged corrected Picarro dataset\n",
    "    df_p_10m = pl.scan_parquet(os.path.join(DATA_DIRECTORY,\"input\", \"picarro.parquet\")) \\\n",
    "        .with_columns(pl.col(\"datetime\").dt.cast_time_unit(\"us\").dt.replace_time_zone(\"UTC\").alias(\"creation_timestamp\")) \\\n",
    "        .sort(\"creation_timestamp\") \\\n",
    "        .with_columns((pl.col(\"CO2_dry\") * picarro_slope + picarro_intercept).alias(\"CO2_corr\")) \\\n",
    "        .groupby_dynamic(\"creation_timestamp\", every='10m') \\\n",
    "        .agg(pl.all().exclude(\"creation_timestamp\").mean()).collect() \\\n",
    "        .select([\"creation_timestamp\", \"CO2_corr\", \"h2o_reported\"]) \\\n",
    "        .with_columns([pl.lit(picarro_slope).alias(\"slope\"),\n",
    "            pl.lit(picarro_intercept).alias(\"intercept\"),\n",
    "            pl.lit(\"Picarro\").alias(\"system_name\"),\n",
    "            pl.lit(0.0).alias(\"diff\")]\n",
    "            ) \\\n",
    "\n",
    "    df_p_10m.write_parquet(os.path.join(DATA_DIRECTORY,\"processed\", \"10m_cal_corr_picarro.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 27)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>DATE</th><th>TIME</th><th>FRAC_DAYS_SINCE_JAN1</th><th>FRAC_HRS_SINCE_JAN1</th><th>JULIAN_DAYS</th><th>EPOCH_TIME</th><th>ALARM_STATUS</th><th>INST_STATUS</th><th>CavityPressure</th><th>CavityTemp</th><th>DasTemp</th><th>EtalonTemp</th><th>species</th><th>OutletValve</th><th>CH4</th><th>CH4_dry</th><th>CO2</th><th>CO2_dry</th><th>h2o_reported</th><th>ch4_base</th><th>ch4_pzt_std</th><th>co2_base</th><th>co2_pzt_std</th><th>wlm1_offset</th><th>wlm2_offset</th><th>datetime</th><th>__index_level_0__</th></tr><tr><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i64</td><td>i64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>datetime[ns]</td><td>i64</td></tr></thead><tbody><tr><td>&quot;2023-06-23&quot;</td><td>&quot;00:00:01.489&quot;</td><td>173.000017</td><td>4152.000414</td><td>174.000017</td><td>1.6875e9</td><td>0</td><td>963</td><td>139.983005</td><td>44.999779</td><td>43.375</td><td>45.151695</td><td>2.0</td><td>22398.639168</td><td>1.96398</td><td>2.009492</td><td>425.117804</td><td>437.149219</td><td>2.186075</td><td>1182.223133</td><td>84.583058</td><td>1085.124996</td><td>71.320269</td><td>-0.079042</td><td>-0.047342</td><td>2023-06-23 00:00:01.489</td><td>13303676</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 27)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ DATE      ┆ TIME      ┆ FRAC_DAYS ┆ FRAC_HRS_ ┆ … ┆ wlm1_offs ┆ wlm2_offs ┆ datetime  ┆ __index_ │\n",
       "│ ---       ┆ ---       ┆ _SINCE_JA ┆ SINCE_JAN ┆   ┆ et        ┆ et        ┆ ---       ┆ level_0_ │\n",
       "│ str       ┆ str       ┆ N1        ┆ 1         ┆   ┆ ---       ┆ ---       ┆ datetime[ ┆ _        │\n",
       "│           ┆           ┆ ---       ┆ ---       ┆   ┆ f64       ┆ f64       ┆ ns]       ┆ ---      │\n",
       "│           ┆           ┆ f64       ┆ f64       ┆   ┆           ┆           ┆           ┆ i64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 2023-06-2 ┆ 00:00:01. ┆ 173.00001 ┆ 4152.0004 ┆ … ┆ -0.079042 ┆ -0.047342 ┆ 2023-06-2 ┆ 13303676 │\n",
       "│ 3         ┆ 489       ┆ 7         ┆ 14        ┆   ┆           ┆           ┆ 3 00:00:0 ┆          │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆ 1.489     ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.scan_parquet(os.path.join(DATA_DIRECTORY,\"input\", \"picarro.parquet\")).head(1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Dry-Wet Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 45)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>creation_timestamp</th><th>system_name</th><th>gmp343_raw</th><th>gmp343_compensated</th><th>gmp343_filtered</th><th>gmp343_temperature</th><th>wxt532_speed_avg</th><th>wxt532_speed_min</th><th>wxt532_speed_max</th><th>wxt532_direction_avg</th><th>wxt532_direction_min</th><th>wxt532_direction_max</th><th>wxt532_last_update_time</th><th>raspi_cpu_usage</th><th>raspi_cpu_temperature</th><th>raspi_disk_usage</th><th>enclosure_bme280_humidity</th><th>enclosure_bme280_pressure</th><th>enclosure_bme280_temperature</th><th>sht45_humidity</th><th>sht45_temperature</th><th>bme280_humidity</th><th>bme280_temperature</th><th>bme280_pressure</th><th>cal_bottle_id</th><th>cal_gmp343_raw</th><th>cal_gmp343_compensated</th><th>cal_gmp343_filtered</th><th>cal_gmp343_temperature</th><th>cal_bme280_temperature</th><th>cal_bme280_humidity</th><th>cal_bme280_pressure</th><th>cal_sht45_temperature</th><th>cal_sht45_humidity</th><th>revision</th><th>receipt_timestamp</th><th>raspi_memory_usage</th><th>wxt532_temperature</th><th>wxt532_heating_voltage</th><th>wxt532_supply_voltage</th><th>wxt532_reference_voltage</th><th>ups_battery_error_detected</th><th>ups_battery_above_voltage_threshold</th><th>ups_battery_is_fully_charged</th><th>ups_powered_by_grid</th></tr><tr><td>datetime[μs, UTC]</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i32</td><td>datetime[ns, UTC]</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>2023-06-01 00:00:00.960 UTC</td><td>&quot;tum-esm-midcos…</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>0.031</td><td>54.5</td><td>0.426</td><td>15.1</td><td>956.48</td><td>33.77</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 45)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ creation_ ┆ system_na ┆ gmp343_ra ┆ gmp343_co ┆ … ┆ ups_batte ┆ ups_batte ┆ ups_batte ┆ ups_powe │\n",
       "│ timestamp ┆ me        ┆ w         ┆ mpensated ┆   ┆ ry_error_ ┆ ry_above_ ┆ ry_is_ful ┆ red_by_g │\n",
       "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ detected  ┆ voltage_t ┆ ly_charge ┆ rid      │\n",
       "│ datetime[ ┆ str       ┆ f64       ┆ f64       ┆   ┆ ---       ┆ hresh…    ┆ d         ┆ ---      │\n",
       "│ μs, UTC]  ┆           ┆           ┆           ┆   ┆ f64       ┆ ---       ┆ ---       ┆ f64      │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆ f64       ┆ f64       ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 2023-06-0 ┆ tum-esm-m ┆ null      ┆ null      ┆ … ┆ null      ┆ null      ┆ null      ┆ null     │\n",
       "│ 1 00:00:0 ┆ idcost-ra ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 0.960 UTC ┆ spi-1     ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 're_escape' from 'polars.utils.various' (/Users/patrickaigner/Documents/PROJECTS/acropolis-visualisation/.venv/lib/python3.11/site-packages/polars/utils/various.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# concat all station specific aggregated dfs   \u001b[39;00m\n\u001b[1;32m     22\u001b[0m df_wet_concat \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mconcat(df_wet_stations, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiagonal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m df_wet_concat\u001b[38;5;241m.\u001b[39mwith_columns(\u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgmp343_temperature\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msht45_humidity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 25\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrh_to_ah\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msht45_humidity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabsolute_temperature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgmp343_temperature\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh2o_ah\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \\\n\u001b[1;32m     27\u001b[0m \u001b[38;5;241m.\u001b[39mwith_columns(pl\u001b[38;5;241m.\u001b[39mstruct([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgmp343_temperature\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msht45_humidity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbme280_pressure\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     28\u001b[0m \u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: pu\u001b[38;5;241m.\u001b[39mrh_to_molar_mixing(x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msht45_humidity\u001b[39m\u001b[38;5;124m'\u001b[39m],pu\u001b[38;5;241m.\u001b[39mabsolute_temperature(x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgmp343_temperature\u001b[39m\u001b[38;5;124m'\u001b[39m]),x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbme280_pressure\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)) \\\n\u001b[1;32m     29\u001b[0m \u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh2o_ppm\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \\\n\u001b[1;32m     30\u001b[0m \u001b[38;5;241m.\u001b[39mwith_columns(pl\u001b[38;5;241m.\u001b[39mstruct([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgmp343_filtered\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgmp343_temperature\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msht45_humidity\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbme280_pressure\u001b[39m\u001b[38;5;124m'\u001b[39m]) \\\n\u001b[1;32m     31\u001b[0m \u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: pu\u001b[38;5;241m.\u001b[39mcalculate_co2dry(x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgmp343_filtered\u001b[39m\u001b[38;5;124m'\u001b[39m],x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgmp343_temperature\u001b[39m\u001b[38;5;124m'\u001b[39m],x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msht45_humidity\u001b[39m\u001b[38;5;124m'\u001b[39m],x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbme280_pressure\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgmp343_dry\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \\\n\u001b[1;32m     33\u001b[0m \u001b[38;5;241m.\u001b[39mselect([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreation_timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgmp343_dry\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh2o_ah\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh2o_ppm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgmp343_temperature\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbme280_pressure\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msht45_humidity\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \\\n\u001b[1;32m     34\u001b[0m \u001b[38;5;241m.\u001b[39mcollect() \\\n\u001b[1;32m     35\u001b[0m \u001b[38;5;241m.\u001b[39mwrite_parquet(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DATA_DIRECTORY, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macropolis_dry.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/Documents/PROJECTS/acropolis-visualisation/.venv/lib/python3.11/site-packages/polars/expr/expr.py:3803\u001b[0m, in \u001b[0;36mapply\u001b[0;34m(self, function, return_dtype, skip_nulls, pass_name, strategy)\u001b[0m\n\u001b[1;32m   3712\u001b[0m \u001b[38;5;129m@unstable\u001b[39m()\n\u001b[1;32m   3713\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mqcut\u001b[39m(\n\u001b[1;32m   3714\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3720\u001b[0m     include_breaks: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   3721\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   3722\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3723\u001b[0m \u001b[38;5;124;03m    Bin continuous values into discrete categories based on their quantiles.\u001b[39;00m\n\u001b[1;32m   3724\u001b[0m \n\u001b[1;32m   3725\u001b[0m \u001b[38;5;124;03m    .. warning::\u001b[39;00m\n\u001b[1;32m   3726\u001b[0m \u001b[38;5;124;03m        This functionality is considered **unstable**. It may be changed\u001b[39;00m\n\u001b[1;32m   3727\u001b[0m \u001b[38;5;124;03m        at any point without it being considered a breaking change.\u001b[39;00m\n\u001b[1;32m   3728\u001b[0m \n\u001b[1;32m   3729\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   3730\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[1;32m   3731\u001b[0m \u001b[38;5;124;03m    quantiles\u001b[39;00m\n\u001b[1;32m   3732\u001b[0m \u001b[38;5;124;03m        Either a list of quantile probabilities between 0 and 1 or a positive\u001b[39;00m\n\u001b[1;32m   3733\u001b[0m \u001b[38;5;124;03m        integer determining the number of bins with uniform probability.\u001b[39;00m\n\u001b[1;32m   3734\u001b[0m \u001b[38;5;124;03m    labels\u001b[39;00m\n\u001b[1;32m   3735\u001b[0m \u001b[38;5;124;03m        Names of the categories. The number of labels must be equal to the number\u001b[39;00m\n\u001b[1;32m   3736\u001b[0m \u001b[38;5;124;03m        of categories.\u001b[39;00m\n\u001b[1;32m   3737\u001b[0m \u001b[38;5;124;03m    left_closed\u001b[39;00m\n\u001b[1;32m   3738\u001b[0m \u001b[38;5;124;03m        Set the intervals to be left-closed instead of right-closed.\u001b[39;00m\n\u001b[1;32m   3739\u001b[0m \u001b[38;5;124;03m    allow_duplicates\u001b[39;00m\n\u001b[1;32m   3740\u001b[0m \u001b[38;5;124;03m        If set to `True`, duplicates in the resulting quantiles are dropped,\u001b[39;00m\n\u001b[1;32m   3741\u001b[0m \u001b[38;5;124;03m        rather than raising a `DuplicateError`. This can happen even with unique\u001b[39;00m\n\u001b[1;32m   3742\u001b[0m \u001b[38;5;124;03m        probabilities, depending on the data.\u001b[39;00m\n\u001b[1;32m   3743\u001b[0m \u001b[38;5;124;03m    include_breaks\u001b[39;00m\n\u001b[1;32m   3744\u001b[0m \u001b[38;5;124;03m        Include a column with the right endpoint of the bin each observation falls\u001b[39;00m\n\u001b[1;32m   3745\u001b[0m \u001b[38;5;124;03m        in. This will change the data type of the output from a\u001b[39;00m\n\u001b[1;32m   3746\u001b[0m \u001b[38;5;124;03m        :class:`Categorical` to a :class:`Struct`.\u001b[39;00m\n\u001b[1;32m   3747\u001b[0m \n\u001b[1;32m   3748\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[1;32m   3749\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[1;32m   3750\u001b[0m \u001b[38;5;124;03m    Expr\u001b[39;00m\n\u001b[1;32m   3751\u001b[0m \u001b[38;5;124;03m        Expression of data type :class:`Categorical` if `include_breaks` is set to\u001b[39;00m\n\u001b[1;32m   3752\u001b[0m \u001b[38;5;124;03m        `False` (default), otherwise an expression of data type :class:`Struct`.\u001b[39;00m\n\u001b[1;32m   3753\u001b[0m \n\u001b[1;32m   3754\u001b[0m \u001b[38;5;124;03m    See Also\u001b[39;00m\n\u001b[1;32m   3755\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[1;32m   3756\u001b[0m \u001b[38;5;124;03m    cut\u001b[39;00m\n\u001b[1;32m   3757\u001b[0m \n\u001b[1;32m   3758\u001b[0m \u001b[38;5;124;03m    Examples\u001b[39;00m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;124;03m    --------\u001b[39;00m\n\u001b[1;32m   3760\u001b[0m \u001b[38;5;124;03m    Divide a column into three categories according to pre-defined quantile\u001b[39;00m\n\u001b[1;32m   3761\u001b[0m \u001b[38;5;124;03m    probabilities.\u001b[39;00m\n\u001b[1;32m   3762\u001b[0m \n\u001b[1;32m   3763\u001b[0m \u001b[38;5;124;03m    >>> df = pl.DataFrame({\"foo\": [-2, -1, 0, 1, 2]})\u001b[39;00m\n\u001b[1;32m   3764\u001b[0m \u001b[38;5;124;03m    >>> df.with_columns(\u001b[39;00m\n\u001b[1;32m   3765\u001b[0m \u001b[38;5;124;03m    ...     pl.col(\"foo\").qcut([0.25, 0.75], labels=[\"a\", \"b\", \"c\"]).alias(\"qcut\")\u001b[39;00m\n\u001b[1;32m   3766\u001b[0m \u001b[38;5;124;03m    ... )\u001b[39;00m\n\u001b[1;32m   3767\u001b[0m \u001b[38;5;124;03m    shape: (5, 2)\u001b[39;00m\n\u001b[1;32m   3768\u001b[0m \u001b[38;5;124;03m    ┌─────┬──────┐\u001b[39;00m\n\u001b[1;32m   3769\u001b[0m \u001b[38;5;124;03m    │ foo ┆ qcut │\u001b[39;00m\n\u001b[1;32m   3770\u001b[0m \u001b[38;5;124;03m    │ --- ┆ ---  │\u001b[39;00m\n\u001b[1;32m   3771\u001b[0m \u001b[38;5;124;03m    │ i64 ┆ cat  │\u001b[39;00m\n\u001b[1;32m   3772\u001b[0m \u001b[38;5;124;03m    ╞═════╪══════╡\u001b[39;00m\n\u001b[1;32m   3773\u001b[0m \u001b[38;5;124;03m    │ -2  ┆ a    │\u001b[39;00m\n\u001b[1;32m   3774\u001b[0m \u001b[38;5;124;03m    │ -1  ┆ a    │\u001b[39;00m\n\u001b[1;32m   3775\u001b[0m \u001b[38;5;124;03m    │ 0   ┆ b    │\u001b[39;00m\n\u001b[1;32m   3776\u001b[0m \u001b[38;5;124;03m    │ 1   ┆ b    │\u001b[39;00m\n\u001b[1;32m   3777\u001b[0m \u001b[38;5;124;03m    │ 2   ┆ c    │\u001b[39;00m\n\u001b[1;32m   3778\u001b[0m \u001b[38;5;124;03m    └─────┴──────┘\u001b[39;00m\n\u001b[1;32m   3779\u001b[0m \n\u001b[1;32m   3780\u001b[0m \u001b[38;5;124;03m    Divide a column into two categories using uniform quantile probabilities.\u001b[39;00m\n\u001b[1;32m   3781\u001b[0m \n\u001b[1;32m   3782\u001b[0m \u001b[38;5;124;03m    >>> df.with_columns(\u001b[39;00m\n\u001b[1;32m   3783\u001b[0m \u001b[38;5;124;03m    ...     pl.col(\"foo\")\u001b[39;00m\n\u001b[1;32m   3784\u001b[0m \u001b[38;5;124;03m    ...     .qcut(2, labels=[\"low\", \"high\"], left_closed=True)\u001b[39;00m\n\u001b[1;32m   3785\u001b[0m \u001b[38;5;124;03m    ...     .alias(\"qcut\")\u001b[39;00m\n\u001b[1;32m   3786\u001b[0m \u001b[38;5;124;03m    ... )\u001b[39;00m\n\u001b[1;32m   3787\u001b[0m \u001b[38;5;124;03m    shape: (5, 2)\u001b[39;00m\n\u001b[1;32m   3788\u001b[0m \u001b[38;5;124;03m    ┌─────┬──────┐\u001b[39;00m\n\u001b[1;32m   3789\u001b[0m \u001b[38;5;124;03m    │ foo ┆ qcut │\u001b[39;00m\n\u001b[1;32m   3790\u001b[0m \u001b[38;5;124;03m    │ --- ┆ ---  │\u001b[39;00m\n\u001b[1;32m   3791\u001b[0m \u001b[38;5;124;03m    │ i64 ┆ cat  │\u001b[39;00m\n\u001b[1;32m   3792\u001b[0m \u001b[38;5;124;03m    ╞═════╪══════╡\u001b[39;00m\n\u001b[1;32m   3793\u001b[0m \u001b[38;5;124;03m    │ -2  ┆ low  │\u001b[39;00m\n\u001b[1;32m   3794\u001b[0m \u001b[38;5;124;03m    │ -1  ┆ low  │\u001b[39;00m\n\u001b[1;32m   3795\u001b[0m \u001b[38;5;124;03m    │ 0   ┆ high │\u001b[39;00m\n\u001b[1;32m   3796\u001b[0m \u001b[38;5;124;03m    │ 1   ┆ high │\u001b[39;00m\n\u001b[1;32m   3797\u001b[0m \u001b[38;5;124;03m    │ 2   ┆ high │\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[38;5;124;03m    └─────┴──────┘\u001b[39;00m\n\u001b[1;32m   3799\u001b[0m \n\u001b[1;32m   3800\u001b[0m \u001b[38;5;124;03m    Add both the category and the breakpoint.\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m \n\u001b[1;32m   3802\u001b[0m \u001b[38;5;124;03m    >>> df.with_columns(\u001b[39;00m\n\u001b[0;32m-> 3803\u001b[0m \u001b[38;5;124;03m    ...     pl.col(\"foo\").qcut([0.25, 0.75], include_breaks=True).alias(\"qcut\")\u001b[39;00m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;124;03m    ... ).unnest(\"qcut\")\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;124;03m    shape: (5, 3)\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;124;03m    ┌─────┬──────┬────────────┐\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m \u001b[38;5;124;03m    │ foo ┆ brk  ┆ foo_bin    │\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;124;03m    │ --- ┆ ---  ┆ ---        │\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m \u001b[38;5;124;03m    │ i64 ┆ f64  ┆ cat        │\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m \u001b[38;5;124;03m    ╞═════╪══════╪════════════╡\u001b[39;00m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;124;03m    │ -2  ┆ -1.0 ┆ (-inf, -1] │\u001b[39;00m\n\u001b[1;32m   3812\u001b[0m \u001b[38;5;124;03m    │ -1  ┆ -1.0 ┆ (-inf, -1] │\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;124;03m    │ 0   ┆ 1.0  ┆ (-1, 1]    │\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m \u001b[38;5;124;03m    │ 1   ┆ 1.0  ┆ (-1, 1]    │\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m \u001b[38;5;124;03m    │ 2   ┆ inf  ┆ (1, inf]   │\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m \u001b[38;5;124;03m    └─────┴──────┴────────────┘\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3818\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(quantiles, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m   3819\u001b[0m         pyexpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pyexpr\u001b[38;5;241m.\u001b[39mqcut_uniform(\n\u001b[1;32m   3820\u001b[0m             quantiles, labels, left_closed, allow_duplicates, include_breaks\n\u001b[1;32m   3821\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/PROJECTS/acropolis-visualisation/.venv/lib/python3.11/site-packages/polars/utils/udfs.py:28\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     TYPE_CHECKING,\n\u001b[1;32m     18\u001b[0m     AbstractSet,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     Union,\n\u001b[1;32m     26\u001b[0m )\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpolars\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvarious\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m re_escape\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Instruction\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 're_escape' from 'polars.utils.various' (/Users/patrickaigner/Documents/PROJECTS/acropolis-visualisation/.venv/lib/python3.11/site-packages/polars/utils/various.py)"
     ]
    }
   ],
   "source": [
    "df_wet_stations = []\n",
    "\n",
    "for id in sensor_id:  \n",
    "    # aggregate data to 1m\n",
    "    df_wet_station = df.filter(pl.col(\"system_name\") == f\"tum-esm-midcost-raspi-{id}\") \\\n",
    "        .select([\"creation_timestamp\", \"system_name\", 'gmp343_filtered','gmp343_temperature','sht45_humidity','bme280_pressure']) \\\n",
    "        .sort(\"creation_timestamp\") \\\n",
    "        .filter(pl.col('gmp343_filtered') > 0) \\\n",
    "        .filter(pl.col('gmp343_temperature') > 0) \\\n",
    "        .filter(pl.col('sht45_humidity') > 0) \\\n",
    "        .filter(pl.col('bme280_pressure') > 0) \\\n",
    "        .groupby_dynamic(\"creation_timestamp\", every='10m')  \\\n",
    "        .agg([\n",
    "            pl.all().exclude([\"creation_timestamp\",\"system_name\"]).mean(),\n",
    "            pl.col(\"system_name\")\n",
    "            ]) \\\n",
    "        .with_columns(pl.col(\"system_name\").list.last()) \\\n",
    "    \n",
    "    df_wet_stations.append(df_wet_station)\n",
    "     \n",
    "    # concat all station specific aggregated dfs   \n",
    "    df_wet_concat = pl.concat(df_wet_stations, how=\"diagonal\")\n",
    "        \n",
    "    df_wet_concat.with_columns(pl.struct(['gmp343_temperature','sht45_humidity']) \\\n",
    "    .apply(lambda x: pu.rh_to_ah(x['sht45_humidity'],pu.absolute_temperature(x['gmp343_temperature'])))\n",
    "    .alias(\"h2o_ah\")) \\\n",
    "    .with_columns(pl.struct(['gmp343_temperature','sht45_humidity','bme280_pressure'])\n",
    "    .apply(lambda x: pu.rh_to_molar_mixing(x['sht45_humidity'],pu.absolute_temperature(x['gmp343_temperature']),x['bme280_pressure']*100)) \\\n",
    "    .alias(\"h2o_ppm\")) \\\n",
    "    .with_columns(pl.struct(['gmp343_filtered','gmp343_temperature','sht45_humidity','bme280_pressure']) \\\n",
    "    .apply(lambda x: pu.calculate_co2dry(x['gmp343_filtered'],x['gmp343_temperature'],x['sht45_humidity'],x['bme280_pressure']*100))\n",
    "    .alias(\"gmp343_dry\")) \\\n",
    "    .select([\"creation_timestamp\", \"system_name\", \"gmp343_dry\", \"h2o_ah\", \"h2o_ppm\",\"gmp343_temperature\",\"bme280_pressure\",\"sht45_humidity\"]) \\\n",
    "    .collect() \\\n",
    "    .write_parquet(os.path.join(DATA_DIRECTORY, \"processed\", \"acropolis_dry.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Calibration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "\n",
    "def average_bottle(data):\n",
    "    data = data.to_list()\n",
    "    #2nd bottle\n",
    "    if 50 < len(data) < 70:\n",
    "        x = data[int(len(data)*0.3):int(len(data)*0.95)]\n",
    "        return sum(x) / len(x)\n",
    "    #1st bottle\n",
    "    elif 70 < len(data) < 130:\n",
    "        x = data[int(len(data)*0.5):int(len(data)*0.95)]\n",
    "        return sum(x) / len(x)\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "def two_point_calibration(measured_values, true_values):\n",
    "    # Check if input lists have length 2\n",
    "    if len(measured_values) != 2 or len(true_values) != 2:\n",
    "        raise ValueError(\"Both measured_values and true_values must have length 2\")\n",
    "\n",
    "    # Calculate calibration parameters (slope and intercept)\n",
    "    # \n",
    "    slope = (true_values[1] - true_values[0]) / (measured_values[1] - measured_values[0])\n",
    "    # y_true = m * y_meas + t\n",
    "    intercept = true_values[0] - slope * measured_values[0]\n",
    "\n",
    "    return slope, intercept\n",
    "\n",
    "def calc_slope(meas_low, meas_high, id_low, id_high):\n",
    "    if (meas_low == None) or (meas_high == None):\n",
    "        return None\n",
    "    \n",
    "    bottles_meas = [meas_low, meas_high]\n",
    "    bottles_true = [df_gas.filter(pl.col(\"Bottle_ID\")== id_low)[\"CO2_dry\"][0],df_gas.filter(pl.col(\"Bottle_ID\")== id_high)[\"CO2_dry\"][0]]\n",
    "\n",
    "    slope, intercept = two_point_calibration(bottles_meas, bottles_true)\n",
    "    \n",
    "    return slope\n",
    "\n",
    "def calc_intercept(meas_low, meas_high, id_low, id_high):\n",
    "    if (meas_low == None) or (meas_high == None):\n",
    "        return None\n",
    "    \n",
    "    bottles_meas = [meas_low, meas_high]\n",
    "    bottles_true = [df_gas.filter(pl.col(\"Bottle_ID\")== id_low)[\"CO2_dry\"][0],df_gas.filter(pl.col(\"Bottle_ID\")== id_high)[\"CO2_dry\"][0]]\n",
    "\n",
    "    slope, intercept = two_point_calibration(bottles_meas, bottles_true)\n",
    "    \n",
    "    return intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.with_columns((pl.col(\"creation_timestamp\").dt.date()).alias(\"date\")).head().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_cal = df.with_columns((pl.col(\"creation_timestamp\").dt.date()).alias(\"date\")).collect().lazy() \\\n",
    ".groupby([pl.col(\"date\"), pl.col(\"system_name\"), pl.col(\"cal_bottle_id\")]) \\\n",
    ".agg([pl.col(\"cal_gmp343_filtered\").drop_nulls(),\n",
    "      pl.col(\"creation_timestamp\").last()]) \\\n",
    ".filter(pl.col(\"cal_bottle_id\") > 0) \\\n",
    "\n",
    "\n",
    "\n",
    "# perform averaging\n",
    "\n",
    "df_cal = df_cal.with_columns(pl.col(\"cal_gmp343_filtered\").apply(lambda x: average_bottle(x)).alias(\"mean_cal\"))\n",
    "\n",
    "# identify low and high span bottle\n",
    "df_cal = df_cal.with_columns([\n",
    "        pl.when(pl.col(\"mean_cal\") < 460).then(pl.col(\"mean_cal\")).otherwise(None).alias(\"mean_cal_low\"),\n",
    "        pl.when(pl.col(\"mean_cal\") > 460).then(pl.col(\"mean_cal\")).otherwise(None).alias(\"mean_cal_high\"),\n",
    "        pl.when(pl.col(\"mean_cal\") < 460).then(pl.col(\"cal_bottle_id\")).otherwise(None).alias(\"id_cal_bottle_low\"),\n",
    "        pl.when(pl.col(\"mean_cal\") > 460).then(pl.col(\"cal_bottle_id\")).otherwise(None).alias(\"id_cal_bottle_high\")\n",
    "    ]) \\\n",
    "    .groupby([pl.col(\"date\").dt.date(), pl.col(\"system_name\")]) \\\n",
    "    .agg([\n",
    "        pl.col(\"mean_cal_low\").sum(),\n",
    "        pl.col(\"mean_cal_high\").sum(),\n",
    "        pl.col(\"id_cal_bottle_low\").sum(),\n",
    "        pl.col(\"id_cal_bottle_high\").sum(),\n",
    "        pl.col(\"creation_timestamp\").last()\n",
    "    ])\n",
    "\n",
    "df_cal.collect()\n",
    "\n",
    "# calculate slope and intercept\n",
    "\n",
    "# filter for days that have a valid calibration for both bottles\n",
    "df_cal = df_cal.sort(pl.col(\"date\")) \\\n",
    "    .filter(pl.col(\"mean_cal_low\") > 0.0 ) \\\n",
    "    .filter(pl.col(\"mean_cal_high\") > 0.0 )\n",
    "\n",
    "# calculate slope\n",
    "df_cal = df_cal.with_columns(pl.struct(['mean_cal_low','mean_cal_high','id_cal_bottle_low','id_cal_bottle_high']) \\\n",
    "    .apply(lambda x: calc_slope(x['mean_cal_low'],x['mean_cal_high'],x['id_cal_bottle_low'],x['id_cal_bottle_high'])) \\\n",
    "    .alias('slope'))\n",
    "\n",
    "# calculate intercept\n",
    "df_cal = df_cal.with_columns(pl.struct(['mean_cal_low','mean_cal_high','id_cal_bottle_low','id_cal_bottle_high']) \\\n",
    "    .apply(lambda x: calc_intercept(x['mean_cal_low'],x['mean_cal_high'],x['id_cal_bottle_low'],x['id_cal_bottle_high'])) \\\n",
    "    .alias('intercept')) \\\n",
    "    .select([\"date\", \"system_name\",\"slope\",\"intercept\", \"creation_timestamp\"])  \\\n",
    "    #.rename({\"creation_timestamp\": \"date\"})\n",
    "\n",
    "df_cal = df_cal.collect()\n",
    "\n",
    "# safe results to parquet\n",
    "df_cal.write_parquet(os.path.join(DATA_DIRECTORY, \"processed\", \"slope_intercept_acropolis.parquet\"))\n",
    "df_cal.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Calibration Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced version for calibration correction\n",
    "df_dry = pl.scan_parquet(os.path.join(DATA_DIRECTORY, \"processed\", \"acropolis_dry.parquet\"))\n",
    "    \n",
    "df_cal = pl.scan_parquet(os.path.join(DATA_DIRECTORY, \"processed\", \"slope_intercept_acropolis.parquet\"))\n",
    "\n",
    "df_p_10m = pl.read_parquet(os.path.join(DATA_DIRECTORY,\"processed\", \"10m_cal_corr_picarro.parquet\"))   \n",
    "df_p_1h = pl.read_parquet(os.path.join(DATA_DIRECTORY,\"processed\", \"1h_cal_corr_picarro.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cal.filter(pl.col(\"system_name\")==\"tum-esm-midcost-raspi-1\").tail().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1h aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce timestamp to date DD.XX.YYYY for measurement dataset and join slope and intercept from df_cal\n",
    "df_date = df_dry.filter(pl.col(\"gmp343_dry\") > 0) \\\n",
    "    .with_columns(pl.col(\"creation_timestamp\").dt.date().alias(\"date\")) \\\n",
    "    .join(df_cal, on = [\"date\",\"system_name\"], how= \"left\")\n",
    "    \n",
    "        \n",
    "l_df_cal_corr =[df_p_1h]\n",
    "\n",
    "for id in sensor_id:  \n",
    "    # broadcast: via backward and forward fill\n",
    "    # calibration correction: via coloumn operation \n",
    "    # aggregation: defined by filter\n",
    "    # offset calculation to reference instrument PICARRO\n",
    "    df_cal_corr = df_date.filter(pl.col(\"system_name\") == f\"tum-esm-midcost-raspi-{id}\") \\\n",
    "    .fill_null(strategy = \"backward\") \\\n",
    "    .fill_null(strategy = \"forward\") \\\n",
    "    .with_columns(((pl.col(\"gmp343_dry\")) * pl.col(\"slope\") + pl.col(\"intercept\")) \\\n",
    "    .alias(\"CO2_corr\")) \\\n",
    "    .sort(\"creation_timestamp\") \\\n",
    "    .groupby_dynamic(\"creation_timestamp\", every='1h')  \\\n",
    "    .agg([\n",
    "        pl.all().exclude([\"creation_timestamp\",\"system_name\"]).mean(),\n",
    "        pl.col(\"system_name\")\n",
    "        ]) \\\n",
    "    .with_columns(pl.col(\"system_name\").list.last()) \\\n",
    "    .collect()\n",
    "        \n",
    "    df_cal_corr = df_cal_corr.join(df_p_1h.select(\"creation_timestamp\", \"CO2_corr\") \\\n",
    "                .rename({\"CO2_corr\": \"temp\"}), on=\"creation_timestamp\", how= \"left\") \\\n",
    "                .with_columns((pl.col(\"CO2_corr\") - pl.col(\"temp\")).alias(\"diff\")) \\\n",
    "                .drop(\"temp\")\n",
    "        \n",
    "    l_df_cal_corr.append(df_cal_corr)\n",
    "        \n",
    "    \n",
    "df_cal_corr_agg = pl.concat(l_df_cal_corr, how=\"diagonal\")\n",
    "df_cal_corr_agg.write_parquet(os.path.join(DATA_DIRECTORY, \"processed\", \"1h_cal_corr_acropolis.parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(df_cal_corr_agg, x=\"creation_timestamp\", y=f\"CO2_corr\", markers=True, title = \"CO2\", color=\"system_name\")\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(df_cal_corr_agg, x=\"creation_timestamp\", y=f\"diff\", markers=True, title = \"CO2\", color=\"system_name\")\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(df_cal_corr_agg, x=\"creation_timestamp\", y=f\"slope\", markers=True, title = \"slope\", color=\"system_name\")\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(df_cal_corr_agg, x=\"creation_timestamp\", y=f\"intercept\", markers=True, title = \"intercept\", color=\"system_name\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10m aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce timestamp to date DD.XX.YYYY for measurement dataset and join slope and intercept from df_cal\n",
    "df_date = df_dry.with_columns(pl.col(\"creation_timestamp\").dt.date().alias(\"date\")) \\\n",
    "    .join(df_cal, on = [\"date\",\"system_name\"], how= \"left\")\n",
    "        \n",
    "l_df_cal_corr =[df_p_10m]\n",
    "\n",
    "for id in sensor_id:  \n",
    "    # broadcast: via backward and forward fill\n",
    "    # calibration correction: via coloumn operation \n",
    "    # aggregation: defined by filter\n",
    "    # offset calculation to reference instrument PICARRO\n",
    "    df_cal_corr = df_date.filter(pl.col(\"system_name\") == f\"tum-esm-midcost-raspi-{id}\") \\\n",
    "    .filter(pl.col(\"gmp343_dry\") > 0) \\\n",
    "    .fill_null(strategy = \"backward\") \\\n",
    "    .fill_null(strategy = \"forward\") \\\n",
    "    .with_columns(((pl.col(\"gmp343_dry\")) * pl.col(\"slope\") + pl.col(\"intercept\")) \\\n",
    "    .alias(\"CO2_corr\")) \\\n",
    "    .sort(\"creation_timestamp\") \\\n",
    "    .groupby_dynamic(\"creation_timestamp\", every='10m')  \\\n",
    "    .agg([\n",
    "        pl.all().exclude([\"creation_timestamp\",\"system_name\"]).mean(),\n",
    "        pl.col(\"system_name\")\n",
    "        ]) \\\n",
    "    .with_columns(pl.col(\"system_name\").list.last()) \\\n",
    "    .collect()\n",
    "        \n",
    "    df_cal_corr = df_cal_corr.join(df_p_10m.select(\"creation_timestamp\", \"CO2_corr\") \\\n",
    "                .rename({\"CO2_corr\": \"temp\"}), on=\"creation_timestamp\", how= \"left\") \\\n",
    "                .with_columns((pl.col(\"CO2_corr\") - pl.col(\"temp\")).alias(\"diff\")) \\\n",
    "                .drop(\"temp\")\n",
    "        \n",
    "    l_df_cal_corr.append(df_cal_corr)\n",
    "        \n",
    "    \n",
    "df_cal_corr_agg = pl.concat(l_df_cal_corr, how=\"diagonal\")\n",
    "df_cal_corr_agg.write_parquet(os.path.join(DATA_DIRECTORY, \"processed\", \"10m_cal_corr_acropolis.parquet\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
