{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from hampel import hampel\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from typing import Literal\n",
    "import glob\n",
    "\n",
    "from utils import ambient_parameter_conversion as apc\n",
    "from utils import calibration_processing as cp\n",
    "\n",
    "DATA_DIRECTORY = os.environ.get(\"DATA_DIRECTORY\")\n",
    "\n",
    "sensor_id = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "\n",
    "# customize pipeline\n",
    "outlier_removal = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load calibration bottle concentrations (preprocessed)\n",
    "df_gas = pl.read_csv(os.path.join(DATA_DIRECTORY,\"input\", \"averaged_gases.csv\"))\n",
    "\n",
    "# load all local chunks for 2024\n",
    "year = 2024\n",
    "measurement_months = []\n",
    "\n",
    "paths = sorted(glob.glob(os.path.join(DATA_DIRECTORY, \"download\", \"measurements\", str(year), \"*.parquet\")), key=os.path.getmtime)\n",
    "\n",
    "for path in paths:\n",
    "    measurement_months.append(pl.scan_parquet(path))\n",
    "\n",
    "df_raw = pl.concat(measurement_months, how=\"diagonal\") \\\n",
    "    .filter(pl.col(\"system_name\") != \"test-sensor\") \\\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract wind data from df_raw\n",
    "df_wind = df_raw.select(pl.col(\"creation_timestamp\", \"system_id\", \"^(wxt532_.*)$\")) \\\n",
    "    .filter(pl.col('wxt532_direction_avg') > 0) \\\n",
    "    .collect()\n",
    "    \n",
    "df_enclosure = df_raw.select(pl.col(\"creation_timestamp\", \"system_id\", \"^(enclosure_.*)$\")) \\\n",
    "    .filter(pl.col('enclosure_bme280_temperature') > 0) \\\n",
    "    .collect()\n",
    "\n",
    "# extract measurement data from df_raw and aggregate to 1m \n",
    "df_1_m = df_raw.sort(\"creation_timestamp\") \\\n",
    "    .select(pl.all().exclude('^wxt532_.*$', '^cal_.*$', '^enclosure_.*$', '^raspi_.*$', '^ups_.*$')) \\\n",
    "    .filter(pl.col('gmp343_filtered') > 0) \\\n",
    "    .filter(pl.col('gmp343_temperature') > 0) \\\n",
    "    .filter(pl.col('sht45_humidity') > 0) \\\n",
    "    .filter(pl.col('bme280_pressure') > 0) \\\n",
    "    .group_by_dynamic(\"creation_timestamp\", every='1m', by= \"system_id\") \\\n",
    "    .agg(pl.all().exclude([\"creation_timestamp\",\"system_id\"]).mean()) \\\n",
    "    .collect()\n",
    "    \n",
    "# extract calibration data from df_raw\n",
    "df_dry_calibration = df_raw.filter(pl.col(\"cal_gmp343_filtered\") > 0) \\\n",
    "    .filter(pl.col(\"cal_gmp343_temperature\") > 0) \\\n",
    "    .filter(pl.col(\"cal_bme280_pressure\") > 0) \\\n",
    "    .with_columns(pl.col(\"cal_sht45_humidity\").fill_null(0.0)) \\\n",
    "    .with_columns(pl.struct(['cal_gmp343_temperature','cal_sht45_humidity','cal_bme280_pressure'])\n",
    "    .map_elements(lambda x: apc.rh_to_molar_mixing(x['cal_sht45_humidity'],apc.absolute_temperature(x['cal_gmp343_temperature']),x['cal_bme280_pressure']*100)) \\\n",
    "    .alias(\"cal_h2o_v%\")) \\\n",
    "    .with_columns(pl.struct(['cal_gmp343_filtered','cal_gmp343_temperature','cal_sht45_humidity','cal_bme280_pressure']) \\\n",
    "    .map_elements(lambda x: apc.calculate_co2dry(x['cal_gmp343_filtered'],x['cal_gmp343_temperature'],x['cal_sht45_humidity'],x['cal_bme280_pressure']*100))\n",
    "    .alias(\"cal_gmp343_dry\")) \\\n",
    "    .select(\"creation_timestamp\",\"system_id\", '^cal_.*$') \\\n",
    "    .filter((pl.col(\"cal_bottle_id\") > 0) & (pl.col(\"cal_bottle_id\") <= df_gas[\"cal_bottle_id\"].max())) \\\n",
    "    .collect()\n",
    "    \n",
    "df_raw = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Dry-Wet Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform dry conversion for measurement data                \n",
    "df_1_m = df_1_m.with_columns(pl.struct(['gmp343_temperature','sht45_humidity']) \\\n",
    "    .map_elements(lambda x: apc.rh_to_ah(x['sht45_humidity'],apc.absolute_temperature(x['gmp343_temperature'])))\n",
    "    .alias(\"h2o_ah\")) \\\n",
    "    .with_columns(pl.struct(['gmp343_temperature','sht45_humidity','bme280_pressure'])\n",
    "    .map_elements(lambda x: (apc.rh_to_molar_mixing(x['sht45_humidity'],apc.absolute_temperature(x['gmp343_temperature']),x['bme280_pressure']*100))*100) \\\n",
    "    .alias(\"h2o_v%\")) \\\n",
    "    .with_columns(pl.struct(['gmp343_filtered','gmp343_temperature','sht45_humidity','bme280_pressure']) \\\n",
    "    .map_elements(lambda x: apc.calculate_co2dry(x['gmp343_filtered'],x['gmp343_temperature'],x['sht45_humidity'],x['bme280_pressure']*100))\n",
    "    .alias(\"gmp343_dry\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_m.tail(3).select(\"creation_timestamp\",\"system_id\",\"gmp343_filtered\", \"h2o_ah\", \"h2o_v%\" ,\"gmp343_dry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_m.write_parquet(os.path.join(DATA_DIRECTORY, \"processed\", \"pipeline\", \"dry_1_min_acropolis.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Calibration Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slope_intercept = df_dry_calibration.join(df_gas.cast({\"cal_bottle_id\": pl.Float64}), on = [\"cal_bottle_id\"], how= \"left\") \\\n",
    "    .with_columns((pl.col(\"creation_timestamp\").dt.date()).alias(\"date\")) \\\n",
    "    .sort(\"date\") \\\n",
    "    .group_by([pl.col(\"date\"), pl.col(\"system_id\"), pl.col(\"cal_bottle_id\")]) \\\n",
    "    .agg([\n",
    "        pl.col(\"cal_gmp343_dry\"),\n",
    "        pl.col(\"cal_bottle_CO2\").last(),\n",
    "        pl.col(\"creation_timestamp\").last(),\n",
    "        ]) \\\n",
    "    .with_columns([pl.col(\"cal_gmp343_dry\").map_elements(lambda x: cp.process_bottle(x))]) \\\n",
    "    .filter(pl.col(\"cal_gmp343_dry\") > 0) \\\n",
    "    .sort(pl.col(\"cal_gmp343_dry\")) \\\n",
    "    .group_by([\"date\", \"system_id\"]) \\\n",
    "    .agg([\n",
    "        pl.col(\"cal_gmp343_dry\"),\n",
    "        pl.col(\"cal_bottle_CO2\"),\n",
    "        pl.col(\"creation_timestamp\").last()\n",
    "        ]) \\\n",
    "    .filter(pl.col(\"cal_gmp343_dry\").list.len() == 2) \\\n",
    "    .with_columns(pl.struct(['cal_gmp343_dry','cal_bottle_CO2']) \\\n",
    "    .map_elements(lambda x: cp.two_point_calibration(x['cal_gmp343_dry'],x['cal_bottle_CO2'])) \\\n",
    "    .alias('slope, intercept')) \\\n",
    "    .with_columns([(pl.col(\"slope, intercept\").list.first()).alias(\"slope\"),\n",
    "                   (pl.col(\"slope, intercept\").list.last()).alias(\"intercept\")\n",
    "                   ]) \\\n",
    "    .select(\"creation_timestamp\", \"system_id\", \"slope\", \"intercept\") \\\n",
    "    .filter(pl.col(\"slope\") > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slope_intercept.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe results to parquet\n",
    "df_slope_intercept.write_parquet(os.path.join(DATA_DIRECTORY, \"processed\", \"pipeline\", \"slope_intercept_acropolis.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_slope_intercept.sort(\"creation_timestamp\").filter((pl.col(\"slope\") > 0.7) & (pl.col(\"slope\") < 1.1)), x=\"creation_timestamp\", y = \"slope\", color = \"system_id\")\n",
    "fig.show()\n",
    "fig = px.histogram(df_slope_intercept.filter((pl.col(\"slope\") > 0.7) & (pl.col(\"slope\") < 1.1)), x=\"slope\", color = \"system_id\")\n",
    "fig.show()\n",
    "fig = px.histogram(df_slope_intercept.filter((pl.col(\"intercept\") < 100) & (pl.col(\"intercept\") > -100)), x=\"intercept\", color = \"system_id\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Calibration Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1m aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_systems = []\n",
    "\n",
    "for id in sensor_id:\n",
    "    df_slope_intercept_id = df_slope_intercept.filter(pl.col(\"system_id\") == id) \\\n",
    "        .sort(\"creation_timestamp\") \\\n",
    "        .drop(\"system_id\")\n",
    "        \n",
    "    df_wind_id = df_wind.filter(pl.col(\"system_id\") == id) \\\n",
    "        .sort(\"creation_timestamp\") \\\n",
    "        .drop(\"system_id\", \"system_name\", \"date\")\n",
    "        \n",
    "    df_enclosure_id = df_enclosure.filter(pl.col(\"system_id\") == id) \\\n",
    "        .sort(\"creation_timestamp\") \\\n",
    "        .drop(\"system_id\", \"system_name\", \"date\")\n",
    "    \n",
    "    df_system = df_1_m.filter(pl.col(\"system_id\") == id) \\\n",
    "        .sort(\"creation_timestamp\") \\\n",
    "        .join_asof(df_slope_intercept_id, on=\"creation_timestamp\", strategy=\"nearest\", tolerance=\"10m\") \\\n",
    "        .join_asof(df_wind_id, on=\"creation_timestamp\", strategy=\"nearest\", tolerance=\"10m\") \\\n",
    "        .join_asof(df_enclosure_id, on=\"creation_timestamp\", strategy=\"nearest\", tolerance=\"10m\") \\\n",
    "        .with_columns([\n",
    "            pl.col(\"slope\").interpolate().alias(\"slope_interpolated\"),\n",
    "            pl.col(\"intercept\").interpolate().alias(\"intercept_interpolated\")\n",
    "            ]) \\\n",
    "        .with_columns([\n",
    "            pl.col(\"slope_interpolated\").forward_fill(),\n",
    "            pl.col(\"intercept_interpolated\").forward_fill()\n",
    "            ]) \\\n",
    "        .with_columns(((pl.col(\"gmp343_dry\")) * pl.col(\"slope_interpolated\") + pl.col(\"intercept_interpolated\")).alias(\"gmp343_corrected\")) \\\n",
    "        .with_columns((pl.col(\"creation_timestamp\").dt.date()).alias(\"date\"))\n",
    "            \n",
    "    \n",
    "    df_systems.append(df_system)\n",
    "        \n",
    "\n",
    "df_1_m = pl.concat(df_systems, how=\"vertical\") \\\n",
    "    .with_columns(pl.struct([\"system_id\"]) \\\n",
    "    .map_elements(lambda x: f\"acropolis-{x['system_id']}\") \\\n",
    "    .alias(\"sys_name_short\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_m.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_m.write_parquet(os.path.join(DATA_DIRECTORY, \"processed\", \"pipeline\", \"calibrated_1_min_acropolis.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10m aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_m.sort(\"creation_timestamp\") \\\n",
    "        .group_by_dynamic(\"creation_timestamp\", every='10m', by=[\"system_id\", \"sys_name_short\"]) \\\n",
    "        .agg(pl.all().exclude([\"creation_timestamp\",\"sys_name_short\"]).mean(),\n",
    "                pl.col(\"gmp343_corrected\").std().alias(\"std\"),\n",
    "                pl.col(\"gmp343_corrected\").var().alias(\"var\")) \\\n",
    "        .write_parquet(os.path.join(DATA_DIRECTORY, \"processed\", \"pipeline\", \"calibrated_10_min_acropolis.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1h aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_m.sort(\"creation_timestamp\") \\\n",
    "        .group_by_dynamic(\"creation_timestamp\", every='1h', by=[\"system_id\", \"sys_name_short\"]) \\\n",
    "        .agg(pl.all().exclude([\"creation_timestamp\",\"sys_name_short\"]).mean(),\n",
    "             pl.col(\"gmp343_corrected\").std().alias(\"std\"),\n",
    "             (pl.col(\"gmp343_temperature\").max() - pl.col(\"gmp343_temperature\").min()).alias(\"gmp343_temperature_change\")) \\\n",
    "        .write_parquet(os.path.join(DATA_DIRECTORY, \"processed\", \"pipeline\", \"calibrated_1_h_acropolis.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renaming and column selection for ICOS cities portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(outlier_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_1_m = pl.read_parquet(os.path.join(DATA_DIRECTORY, \"processed\", \"1m_cal_corr_acropolis.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns to be present in the ICOS cities portal product\n",
    "selected_columns = [\"creation_timestamp\", \"system_id\", \"sys_name_short\", \"gmp343_corrected\",\"h2o_v%\", \"wxt532_speed_avg\", \"wxt532_direction_avg\"]\n",
    "\n",
    "df_1_m = df_1_m.select(selected_columns) \\\n",
    "    .rename({\"gmp343_corrected\": \"co2\", \"h2o_v%\":\"h2o\", \"wxt532_speed_avg\":\"ws\",\"wxt532_direction_avg\":\"wd\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_m.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outlier Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_systems = []\n",
    "n_sigma = 2.0\n",
    "\n",
    "for id in sensor_id:\n",
    "    df_filtered = df_1_m.filter(pl.col(\"system_id\")==id) \\\n",
    "        .cast({\"co2\": pl.Float32}) \\\n",
    "        .filter(pl.col(\"co2\") > 0)\n",
    "    \n",
    "    # Convert CO2 column to pandas series \n",
    "    data = df_filtered.get_column(\"co2\").to_pandas()\n",
    "      \n",
    "    # Apply the Hampel filter  \n",
    "    result = hampel(data, window_size=120, n_sigma=n_sigma)\n",
    "    \n",
    "    # Print share of detected spikes\n",
    "    print(f\"System ID: {id}, Detected spikes: {(len(result.outlier_indices) / len(data)):.4f}\")\n",
    "    \n",
    "    # Create column \"OriginalFlag\" = 389 indicating local contamination\n",
    "    df_system = df_filtered.with_columns((pl.from_pandas(result.filtered_data)).alias(\"co2_hampel_filtered\")) \\\n",
    "        .with_columns(pl.when(pl.col(\"co2\").ne(pl.col(\"co2_hampel_filtered\"))).then(pl.lit(185)).otherwise(pl.lit(0)).alias(\"OriginalFlag\")) \\\n",
    "        .drop(\"co2_hampel_filtered\")\n",
    "    \n",
    "    df_systems.append(df_system)\n",
    "    \n",
    "df_1_m_spike_detected = pl.concat(df_systems, how=\"vertical\")\n",
    "\n",
    "# Option to add additional OriginalFlags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_checkpoint = df_1_m_spike_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_m_spike_detected = df_1_m_spike_detected.with_columns(pl.when(pl.col(\"OriginalFlag\") > 0).then(pl.lit('K')).otherwise(pl.lit('O')).alias(\"Flag\"))\n",
    "\n",
    "# save a 1m product for ICOS cities portal\n",
    "df_1_m_spike_detected.write_parquet(os.path.join(DATA_DIRECTORY, \"processed\", \"pipeline\", f\"flagged_1_min_nsigma_{n_sigma}_acropolis.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_m_spike_detected.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a 1h product for ICOS cities portal\n",
    "df_1_h_despiked = df_1_m_spike_detected.sort(\"creation_timestamp\") \\\n",
    "        .filter(pl.col(\"Flag\") == 'O') \\\n",
    "        .drop(\"Flag\", \"OriginalFlag\") \\\n",
    "        .group_by_dynamic(\"creation_timestamp\", every='1h', by=[\"system_id\", \"sys_name_short\"]) \\\n",
    "        .agg(pl.all().exclude([\"creation_timestamp\",\"sys_name_short\"]).mean(),\n",
    "             pl.col(\"co2\").std().alias(\"Stdev\"),\n",
    "             pl.col(\"co2\").count().alias(\"NbPoints\")) \\\n",
    "        .with_columns(pl.when(pl.col(\"NbPoints\") < 40).then(pl.lit(389)).otherwise(pl.lit(0)).alias(\"OriginalFlag\")) \\\n",
    "        .with_columns(\n",
    "                pl.when(pl.col(\"OriginalFlag\") > 0).then(pl.lit('K')).otherwise(pl.lit('O')).alias(\"Flag\"),\n",
    "                (pl.col(\"creation_timestamp\") + timedelta(minutes=30)))\n",
    "             \n",
    "df_1_h_despiked.write_parquet(os.path.join(DATA_DIRECTORY, \"processed\", \"pipeline\", f\"flagged_1_h_nsigma_{n_sigma}_acropolis.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1_h_despiked.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Despiked Data with Continous Error Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_1_h_despiked= pl.read_parquet(os.path.join(DATA_DIRECTORY, \"processed\", \"1h_level_1_cities_portal.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime(2024, 7, 1, 0, 0, 0).replace(tzinfo=timezone.utc)\n",
    "end_date = datetime(2024, 11, 30, 23, 59, 59).replace(tzinfo=timezone.utc)\n",
    "\n",
    "def create_figure(df, system_name:str, start_date, end_date, color:Literal[\"red\", \"blue\", \"green\"]):\n",
    "    \n",
    "    df_plot = df.filter(pl.col(\"creation_timestamp\").is_between(start_date, end_date)) \\\n",
    "        .filter(pl.col(\"sys_name_short\")==system_name) \\\n",
    "        .with_columns(upper = pl.col(\"co2\") + pl.col(\"Stdev\"),\n",
    "                    lower = pl.col(\"co2\") - pl.col(\"Stdev\"))\n",
    "    \n",
    "    \n",
    "    if color=='red':\n",
    "        color_set = ('#b91c1c', 'rgba(239, 68, 68, 0.3)')\n",
    "    if color=='blue':\n",
    "        color_set = ('#1d4ed8','rgba(59, 131, 246, 0.3)')\n",
    "    if color=='green':\n",
    "        color_set = ('#15803d','rgba(34, 197, 94, 0.3)')\n",
    "    \n",
    "    return [\n",
    "        go.Scatter(\n",
    "            name=system_name,\n",
    "            x=df_plot[\"creation_timestamp\"],\n",
    "            y=df_plot[\"co2\"],\n",
    "            mode='lines',\n",
    "            line=dict(color=color_set[0]),\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            name='Upper Bound',\n",
    "            x=df_plot[\"creation_timestamp\"],\n",
    "            y=df_plot[\"upper\"],\n",
    "            mode='lines',\n",
    "            marker=dict(color=color_set[1]),\n",
    "            line=dict(width=0),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            name='Lower Bound',\n",
    "            x=df_plot[\"creation_timestamp\"],\n",
    "            y=df_plot[\"lower\"],\n",
    "            marker=dict(color=color_set[1]),\n",
    "            line=dict(width=0),\n",
    "            mode='lines',\n",
    "            fillcolor=color_set[1],\n",
    "            fill='tonexty',\n",
    "            showlegend=False\n",
    "        )\n",
    "    ]\n",
    "  \n",
    "figures = create_figure(df_1_h_despiked, \"acropolis-14\", start_date, end_date, color=\"red\") \\\n",
    "    + create_figure(df_1_h_despiked, \"acropolis-7\", start_date, end_date, color=\"green\") \\\n",
    "    #+ create_figure(df_1_h_despiked, \"acropolis-6\", start_date, end_date, color=\"blue\")\n",
    "\n",
    "fig = go.Figure(figures)\n",
    "fig.update_layout(\n",
    "    yaxis_title='CO2 (ppm)',\n",
    "    xaxis_title='UTC Time (hourly aggregated)',\n",
    "    title='Continuous, variable value error bars',\n",
    "    hovermode=\"x\"\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
