{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from datetime import timezone\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import plotly.express as px\n",
    "\n",
    "from download import extract\n",
    "from utils import processing_utils as pu\n",
    "\n",
    "DATA_DIRECTORY = os.environ.get(\"DATA_DIRECTORY\")\n",
    "PICARRO_DATA_DIRECTORY = os.environ.get(\"PICARRO_DATA_DIRECTORY\")\n",
    "\n",
    "sensor_id = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,18,20]\n",
    "\n",
    "# customize pipeline\n",
    "download_files = True\n",
    "merge_picarro_files = False\n",
    "\n",
    "# load calibration bottle concentrations (preprocessed)\n",
    "df_gas = pl.read_csv(os.path.join(DATA_DIRECTORY,\"input\", \"averaged_gases.csv\"))\n",
    "# load local db: acropolis.parquet\n",
    "df = pl.scan_parquet(os.path.join(DATA_DIRECTORY, \"download\", \"acropolis.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download to local db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download from hermes database\n",
    "# Use Download/download_from_hermes notebook\n",
    "if download_files:\n",
    "    component = extract.Extract()\n",
    "    result = component.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Picarro Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if merge_picarro_files:\n",
    "    filenames = glob.glob(PICARRO_DATA_DIRECTORY + \"/*/*/*.dat\")\n",
    "\n",
    "    # read all *.dat picarro measurement files and add to single db\n",
    "    df_list = []\n",
    "    for filename in filenames:\n",
    "        df_list.append(pd.read_csv(filename,sep='\\s+'))\n",
    "\n",
    "    df_p_files = pd.concat(df_list, ignore_index=True)\n",
    "    df_p_files[\"datetime\"] = pd.to_datetime((df_p_files['DATE'] + ' ' + df_p_files['TIME']))\n",
    "    df_p_files.sort_values(by='datetime', inplace = True)\n",
    "\n",
    "    df_p_files.to_parquet(path = os.path.join(DATA_DIRECTORY, \"input\", \"picarro.parquet\"))\n",
    "\n",
    "    #Calibration\n",
    "\n",
    "    # TODO: Add ability for multiple calibration dates\n",
    "    # before 23.10\n",
    "    # picarro_slope = 1.0061589132696314\n",
    "    # picarro_intercept = 0.14607153970888476\n",
    "\n",
    "    # after 23.10\n",
    "    #picarro_slope = 1.006374633215469\n",
    "    #picarro_intercept = 0.0709482571842841\n",
    "    \n",
    "    #after 18.12\n",
    "    picarro_slope = 1.0060429925902534 \n",
    "    picarro_intercept = 0.09305508001614271\n",
    "\n",
    "    #1h averaged corrected Picarro dataset\n",
    "    df_p_1h = pl.scan_parquet(os.path.join(DATA_DIRECTORY,\"input\", \"picarro.parquet\")) \\\n",
    "        .with_columns(pl.col(\"datetime\").dt.cast_time_unit(\"us\").dt.replace_time_zone(\"UTC\").alias(\"creation_timestamp\")) \\\n",
    "        .sort(\"creation_timestamp\") \\\n",
    "        .with_columns((pl.col(\"CO2_dry\") * picarro_slope + picarro_intercept).alias(\"CO2_corr\")) \\\n",
    "        .groupby_dynamic(\"creation_timestamp\", every='1h') \\\n",
    "        .agg(pl.all().exclude(\"creation_timestamp\").mean()).collect() \\\n",
    "        .select([\"creation_timestamp\", \"CO2_corr\", \"h2o_reported\"]) \\\n",
    "        .with_columns([pl.lit(picarro_slope).alias(\"slope\"),\n",
    "            pl.lit(picarro_intercept).alias(\"intercept\"),\n",
    "            pl.lit(\"Picarro\").alias(\"system_name\"),\n",
    "            pl.lit(0.0).alias(\"diff\")]\n",
    "            ) \\\n",
    "\n",
    "    df_p_1h.write_parquet(os.path.join(DATA_DIRECTORY,\"processed\", \"1h_cal_corr_picarro.parquet\"))\n",
    "\n",
    "    #10m averaged corrected Picarro dataset\n",
    "    df_p_10m = pl.scan_parquet(os.path.join(DATA_DIRECTORY,\"input\", \"picarro.parquet\")) \\\n",
    "        .with_columns(pl.col(\"datetime\").dt.cast_time_unit(\"us\").dt.replace_time_zone(\"UTC\").alias(\"creation_timestamp\")) \\\n",
    "        .sort(\"creation_timestamp\") \\\n",
    "        .with_columns((pl.col(\"CO2_dry\") * picarro_slope + picarro_intercept).alias(\"CO2_corr\")) \\\n",
    "        .groupby_dynamic(\"creation_timestamp\", every='10m') \\\n",
    "        .agg(pl.all().exclude(\"creation_timestamp\").mean()).collect() \\\n",
    "        .select([\"creation_timestamp\", \"CO2_corr\", \"h2o_reported\"]) \\\n",
    "        .with_columns([pl.lit(picarro_slope).alias(\"slope\"),\n",
    "            pl.lit(picarro_intercept).alias(\"intercept\"),\n",
    "            pl.lit(\"Picarro\").alias(\"system_name\"),\n",
    "            pl.lit(0.0).alias(\"diff\")]\n",
    "            ) \\\n",
    "\n",
    "    df_p_10m.write_parquet(os.path.join(DATA_DIRECTORY,\"processed\", \"10m_cal_corr_picarro.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.scan_parquet(os.path.join(DATA_DIRECTORY,\"input\", \"picarro.parquet\")).head(1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Dry-Wet Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wet_stations = []\n",
    "\n",
    "for id in sensor_id:  \n",
    "    # aggregate data to 1m\n",
    "    df_wet_station = df.filter(pl.col(\"system_name\") == f\"tum-esm-midcost-raspi-{id}\") \\\n",
    "        .select([\"creation_timestamp\", \"system_name\", 'gmp343_filtered','gmp343_temperature','sht45_humidity','bme280_pressure']) \\\n",
    "        .sort(\"creation_timestamp\") \\\n",
    "        .filter(pl.col('gmp343_filtered') > 0) \\\n",
    "        .filter(pl.col('gmp343_temperature') > 0) \\\n",
    "        .filter(pl.col('sht45_humidity') > 0) \\\n",
    "        .filter(pl.col('bme280_pressure') > 0) \\\n",
    "        .groupby_dynamic(\"creation_timestamp\", every='10m')  \\\n",
    "        .agg([\n",
    "            pl.all().exclude([\"creation_timestamp\",\"system_name\"]).mean(),\n",
    "            pl.col(\"system_name\")\n",
    "            ]) \\\n",
    "        .with_columns(pl.col(\"system_name\").list.last()) \\\n",
    "    \n",
    "    df_wet_stations.append(df_wet_station)\n",
    "     \n",
    "    # concat all station specific aggregated dfs   \n",
    "    df_wet_concat = pl.concat(df_wet_stations, how=\"diagonal\")\n",
    "        \n",
    "    df_wet_concat.with_columns(pl.struct(['gmp343_temperature','sht45_humidity']) \\\n",
    "    .apply(lambda x: pu.rh_to_ah(x['sht45_humidity'],pu.absolute_temperature(x['gmp343_temperature'])))\n",
    "    .alias(\"h2o_ah\")) \\\n",
    "    .with_columns(pl.struct(['gmp343_temperature','sht45_humidity','bme280_pressure'])\n",
    "    .apply(lambda x: pu.rh_to_molar_mixing(x['sht45_humidity'],pu.absolute_temperature(x['gmp343_temperature']),x['bme280_pressure']*100)) \\\n",
    "    .alias(\"h2o_ppm\")) \\\n",
    "    .with_columns(pl.struct(['gmp343_filtered','gmp343_temperature','sht45_humidity','bme280_pressure']) \\\n",
    "    .apply(lambda x: pu.calculate_co2dry(x['gmp343_filtered'],x['gmp343_temperature'],x['sht45_humidity'],x['bme280_pressure']*100))\n",
    "    .alias(\"gmp343_dry\")) \\\n",
    "    .select([\"creation_timestamp\", \"system_name\", \"gmp343_dry\", \"h2o_ah\", \"h2o_ppm\",\"gmp343_temperature\",\"bme280_pressure\",\"sht45_humidity\"]) \\\n",
    "    .collect() \\\n",
    "    .write_parquet(os.path.join(DATA_DIRECTORY, \"processed\", \"acropolis_dry.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Calibration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions\n",
    "\n",
    "def average_bottle(data):\n",
    "    data = data.to_list()\n",
    "    #2nd bottle\n",
    "    if 50 < len(data) < 70:\n",
    "        x = data[int(len(data)*0.3):int(len(data)*0.95)]\n",
    "        return sum(x) / len(x)\n",
    "    #1st bottle\n",
    "    elif 70 < len(data) < 130:\n",
    "        x = data[int(len(data)*0.5):int(len(data)*0.95)]\n",
    "        return sum(x) / len(x)\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "def two_point_calibration(measured_values, true_values):\n",
    "    # Check if input lists have length 2\n",
    "    if len(measured_values) != 2 or len(true_values) != 2:\n",
    "        raise ValueError(\"Both measured_values and true_values must have length 2\")\n",
    "\n",
    "    # Calculate calibration parameters (slope and intercept)\n",
    "    # \n",
    "    slope = (true_values[1] - true_values[0]) / (measured_values[1] - measured_values[0])\n",
    "    # y_true = m * y_meas + t\n",
    "    intercept = true_values[0] - slope * measured_values[0]\n",
    "\n",
    "    return slope, intercept\n",
    "\n",
    "def calc_slope(meas_low, meas_high, id_low, id_high):\n",
    "    if (meas_low == None) or (meas_high == None):\n",
    "        return None\n",
    "    \n",
    "    bottles_meas = [meas_low, meas_high]\n",
    "    bottles_true = [df_gas.filter(pl.col(\"Bottle_ID\")== id_low)[\"CO2_dry\"][0],df_gas.filter(pl.col(\"Bottle_ID\")== id_high)[\"CO2_dry\"][0]]\n",
    "\n",
    "    slope, intercept = two_point_calibration(bottles_meas, bottles_true)\n",
    "    \n",
    "    return slope\n",
    "\n",
    "def calc_intercept(meas_low, meas_high, id_low, id_high):\n",
    "    if (meas_low == None) or (meas_high == None):\n",
    "        return None\n",
    "    \n",
    "    bottles_meas = [meas_low, meas_high]\n",
    "    bottles_true = [df_gas.filter(pl.col(\"Bottle_ID\")== id_low)[\"CO2_dry\"][0],df_gas.filter(pl.col(\"Bottle_ID\")== id_high)[\"CO2_dry\"][0]]\n",
    "\n",
    "    slope, intercept = two_point_calibration(bottles_meas, bottles_true)\n",
    "    \n",
    "    return intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.with_columns((pl.col(\"creation_timestamp\").dt.date()).alias(\"date\")).head().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_cal = df.with_columns((pl.col(\"creation_timestamp\").dt.date()).alias(\"date\")).collect().lazy() \\\n",
    ".groupby([pl.col(\"date\"), pl.col(\"system_name\"), pl.col(\"cal_bottle_id\")]) \\\n",
    ".agg([pl.col(\"cal_gmp343_filtered\").drop_nulls(),\n",
    "      pl.col(\"creation_timestamp\").last()]) \\\n",
    ".filter(pl.col(\"cal_bottle_id\") > 0) \\\n",
    "\n",
    "\n",
    "\n",
    "# perform averaging\n",
    "\n",
    "df_cal = df_cal.with_columns(pl.col(\"cal_gmp343_filtered\").apply(lambda x: average_bottle(x)).alias(\"mean_cal\"))\n",
    "\n",
    "# identify low and high span bottle\n",
    "df_cal = df_cal.with_columns([\n",
    "        pl.when(pl.col(\"mean_cal\") < 460).then(pl.col(\"mean_cal\")).otherwise(None).alias(\"mean_cal_low\"),\n",
    "        pl.when(pl.col(\"mean_cal\") > 460).then(pl.col(\"mean_cal\")).otherwise(None).alias(\"mean_cal_high\"),\n",
    "        pl.when(pl.col(\"mean_cal\") < 460).then(pl.col(\"cal_bottle_id\")).otherwise(None).alias(\"id_cal_bottle_low\"),\n",
    "        pl.when(pl.col(\"mean_cal\") > 460).then(pl.col(\"cal_bottle_id\")).otherwise(None).alias(\"id_cal_bottle_high\")\n",
    "    ]) \\\n",
    "    .groupby([pl.col(\"date\").dt.date(), pl.col(\"system_name\")]) \\\n",
    "    .agg([\n",
    "        pl.col(\"mean_cal_low\").sum(),\n",
    "        pl.col(\"mean_cal_high\").sum(),\n",
    "        pl.col(\"id_cal_bottle_low\").sum(),\n",
    "        pl.col(\"id_cal_bottle_high\").sum(),\n",
    "        pl.col(\"creation_timestamp\").last()\n",
    "    ])\n",
    "\n",
    "df_cal.collect()\n",
    "\n",
    "# calculate slope and intercept\n",
    "\n",
    "# filter for days that have a valid calibration for both bottles\n",
    "df_cal = df_cal.sort(pl.col(\"date\")) \\\n",
    "    .filter(pl.col(\"mean_cal_low\") > 0.0 ) \\\n",
    "    .filter(pl.col(\"mean_cal_high\") > 0.0 )\n",
    "\n",
    "# calculate slope\n",
    "df_cal = df_cal.with_columns(pl.struct(['mean_cal_low','mean_cal_high','id_cal_bottle_low','id_cal_bottle_high']) \\\n",
    "    .apply(lambda x: calc_slope(x['mean_cal_low'],x['mean_cal_high'],x['id_cal_bottle_low'],x['id_cal_bottle_high'])) \\\n",
    "    .alias('slope'))\n",
    "\n",
    "# calculate intercept\n",
    "df_cal = df_cal.with_columns(pl.struct(['mean_cal_low','mean_cal_high','id_cal_bottle_low','id_cal_bottle_high']) \\\n",
    "    .apply(lambda x: calc_intercept(x['mean_cal_low'],x['mean_cal_high'],x['id_cal_bottle_low'],x['id_cal_bottle_high'])) \\\n",
    "    .alias('intercept')) \\\n",
    "    .select([\"date\", \"system_name\",\"slope\",\"intercept\", \"creation_timestamp\"])  \\\n",
    "    #.rename({\"creation_timestamp\": \"date\"})\n",
    "\n",
    "df_cal = df_cal.collect()\n",
    "\n",
    "# safe results to parquet\n",
    "df_cal.write_parquet(os.path.join(DATA_DIRECTORY, \"processed\", \"slope_intercept_acropolis.parquet\"))\n",
    "df_cal.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Calibration Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced version for calibration correction\n",
    "df_dry = pl.scan_parquet(os.path.join(DATA_DIRECTORY, \"processed\", \"acropolis_dry.parquet\"))\n",
    "    \n",
    "df_cal = pl.scan_parquet(os.path.join(DATA_DIRECTORY, \"processed\", \"slope_intercept_acropolis.parquet\"))\n",
    "\n",
    "df_p_10m = pl.read_parquet(os.path.join(DATA_DIRECTORY,\"processed\", \"10m_cal_corr_picarro.parquet\"))   \n",
    "df_p_1h = pl.read_parquet(os.path.join(DATA_DIRECTORY,\"processed\", \"1h_cal_corr_picarro.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cal.filter(pl.col(\"system_name\")==\"tum-esm-midcost-raspi-1\").tail().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1h aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce timestamp to date DD.XX.YYYY for measurement dataset and join slope and intercept from df_cal\n",
    "df_date = df_dry.filter(pl.col(\"gmp343_dry\") > 0) \\\n",
    "    .with_columns(pl.col(\"creation_timestamp\").dt.date().alias(\"date\")) \\\n",
    "    .join(df_cal, on = [\"date\",\"system_name\"], how= \"left\")\n",
    "    \n",
    "        \n",
    "l_df_cal_corr =[df_p_1h]\n",
    "\n",
    "for id in sensor_id:  \n",
    "    # broadcast: via backward and forward fill\n",
    "    # calibration correction: via coloumn operation \n",
    "    # aggregation: defined by filter\n",
    "    # offset calculation to reference instrument PICARRO\n",
    "    df_cal_corr = df_date.filter(pl.col(\"system_name\") == f\"tum-esm-midcost-raspi-{id}\") \\\n",
    "    .fill_null(strategy = \"backward\") \\\n",
    "    .fill_null(strategy = \"forward\") \\\n",
    "    .with_columns(((pl.col(\"gmp343_dry\")) * pl.col(\"slope\") + pl.col(\"intercept\")) \\\n",
    "    .alias(\"CO2_corr\")) \\\n",
    "    .sort(\"creation_timestamp\") \\\n",
    "    .groupby_dynamic(\"creation_timestamp\", every='1h')  \\\n",
    "    .agg([\n",
    "        pl.all().exclude([\"creation_timestamp\",\"system_name\"]).mean(),\n",
    "        pl.col(\"system_name\")\n",
    "        ]) \\\n",
    "    .with_columns(pl.col(\"system_name\").list.last()) \\\n",
    "    .collect()\n",
    "        \n",
    "    df_cal_corr = df_cal_corr.join(df_p_1h.select(\"creation_timestamp\", \"CO2_corr\") \\\n",
    "                .rename({\"CO2_corr\": \"temp\"}), on=\"creation_timestamp\", how= \"left\") \\\n",
    "                .with_columns((pl.col(\"CO2_corr\") - pl.col(\"temp\")).alias(\"diff\")) \\\n",
    "                .drop(\"temp\")\n",
    "        \n",
    "    l_df_cal_corr.append(df_cal_corr)\n",
    "        \n",
    "    \n",
    "df_cal_corr_agg = pl.concat(l_df_cal_corr, how=\"diagonal\")\n",
    "df_cal_corr_agg.write_parquet(os.path.join(DATA_DIRECTORY, \"processed\", \"1h_cal_corr_acropolis.parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.line(df_cal_corr_agg, x=\"creation_timestamp\", y=f\"CO2_corr\", markers=True, title = \"CO2\", color=\"system_name\")\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(df_cal_corr_agg, x=\"creation_timestamp\", y=f\"diff\", markers=True, title = \"CO2\", color=\"system_name\")\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(df_cal_corr_agg, x=\"creation_timestamp\", y=f\"slope\", markers=True, title = \"slope\", color=\"system_name\")\n",
    "fig.show()\n",
    "\n",
    "fig = px.line(df_cal_corr_agg, x=\"creation_timestamp\", y=f\"intercept\", markers=True, title = \"intercept\", color=\"system_name\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10m aggregated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce timestamp to date DD.XX.YYYY for measurement dataset and join slope and intercept from df_cal\n",
    "df_date = df_dry.with_columns(pl.col(\"creation_timestamp\").dt.date().alias(\"date\")) \\\n",
    "    .join(df_cal, on = [\"date\",\"system_name\"], how= \"left\")\n",
    "        \n",
    "l_df_cal_corr =[df_p_10m]\n",
    "\n",
    "for id in sensor_id:  \n",
    "    # broadcast: via backward and forward fill\n",
    "    # calibration correction: via coloumn operation \n",
    "    # aggregation: defined by filter\n",
    "    # offset calculation to reference instrument PICARRO\n",
    "    df_cal_corr = df_date.filter(pl.col(\"system_name\") == f\"tum-esm-midcost-raspi-{id}\") \\\n",
    "    .filter(pl.col(\"gmp343_dry\") > 0) \\\n",
    "    .fill_null(strategy = \"backward\") \\\n",
    "    .fill_null(strategy = \"forward\") \\\n",
    "    .with_columns(((pl.col(\"gmp343_dry\")) * pl.col(\"slope\") + pl.col(\"intercept\")) \\\n",
    "    .alias(\"CO2_corr\")) \\\n",
    "    .sort(\"creation_timestamp\") \\\n",
    "    .groupby_dynamic(\"creation_timestamp\", every='10m')  \\\n",
    "    .agg([\n",
    "        pl.all().exclude([\"creation_timestamp\",\"system_name\"]).mean(),\n",
    "        pl.col(\"system_name\")\n",
    "        ]) \\\n",
    "    .with_columns(pl.col(\"system_name\").list.last()) \\\n",
    "    .collect()\n",
    "        \n",
    "    df_cal_corr = df_cal_corr.join(df_p_10m.select(\"creation_timestamp\", \"CO2_corr\") \\\n",
    "                .rename({\"CO2_corr\": \"temp\"}), on=\"creation_timestamp\", how= \"left\") \\\n",
    "                .with_columns((pl.col(\"CO2_corr\") - pl.col(\"temp\")).alias(\"diff\")) \\\n",
    "                .drop(\"temp\")\n",
    "        \n",
    "    l_df_cal_corr.append(df_cal_corr)\n",
    "        \n",
    "    \n",
    "df_cal_corr_agg = pl.concat(l_df_cal_corr, how=\"diagonal\")\n",
    "df_cal_corr_agg.write_parquet(os.path.join(DATA_DIRECTORY, \"processed\", \"10m_cal_corr_acropolis.parquet\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
